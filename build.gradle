import groovy.json.JsonSlurper

apply plugin: 'wrapper'
apply plugin: 'distribution'

if (JavaVersion.current().isJava8Compatible()) {
  allprojects {
    tasks.withType(Javadoc) {
      options.addStringOption('Xdoclint:none', '-quiet')
    }
  }
}

buildscript {
  repositories {
    maven { url "https://plugins.gradle.org/m2" }
    jcenter()
    maven { url "https://oss.sonatype.org/content/repositories/staging" }
  }
  dependencies {
    classpath 'io.snappydata:gradle-scalatest:0.13-1'
    classpath 'org.github.ngbinh.scalastyle:gradle-scalastyle-plugin_2.10:0.8.2'
    classpath 'com.github.jengelman.gradle.plugins:shadow:1.2.3'
  }
}

allprojects {
  // We want to see all test results.  This is equivalent to setting --continue
  // on the command line.
  gradle.startParameter.continueOnFailure = true

  repositories {
    jcenter()
    maven { url "https://oss.sonatype.org/content/repositories/snapshots" }
    maven { url "http://repository.snappydata.io:8089/repository/internal" }
    maven { url "http://repository.snappydata.io:8089/repository/snapshots" }
    maven { url "https://repository.apache.org/content/repositories/releases" }
    maven { url "https://repository.jboss.org/nexus/content/repositories/releases" }
    maven { url "https://repo.eclipse.org/content/repositories/paho-releases" }
    maven { url "https://repository.cloudera.com/artifactory/cloudera-repos" }
    maven { url "https://oss.sonatype.org/content/repositories/orgspark-project-1113" }
    maven { url "http://repository.mapr.com/maven" }
    maven { url "https://repo.spring.io/libs-release" }
    maven { url "http://maven.twttr.com" }
    maven { url "http://repository.apache.org/snapshots" }
  }

  apply plugin: 'java'
  apply plugin: 'maven'
  apply plugin: 'scalaStyle'
  apply plugin: 'idea'
  apply plugin: 'eclipse'

  group = 'io.snappydata'
  version = '0.5.2-SNAPSHOT'

  // apply compiler options
  tasks.withType(JavaCompile) {
    options.encoding = 'UTF-8'
    options.incremental = true
    options.compilerArgs << '-Xlint:-serial,-path,-deprecation,-unchecked,-rawtypes'
    options.compilerArgs << '-XDignore.symbol.file'
    options.fork = true
    options.forkOptions.executable = 'javac'
  }
  tasks.withType(ScalaCompile) {
    // scalaCompileOptions.optimize = true
    scalaCompileOptions.useAnt = false
    scalaCompileOptions.deprecation = false
    scalaCompileOptions.additionalParameters = [ '-feature' ]
    options.encoding = 'UTF-8'
  }

  javadoc.options.charSet = 'UTF-8'

  gradle.taskGraph.whenReady( { graph ->
    tasks.withType(Tar).each { tar ->
      tar.compression = Compression.GZIP
      tar.extension = 'tar.gz'
    }
  })

  ext {
    productName = 'SnappyData'
    scalaBinaryVersion = '2.10'
    scalaVersion = scalaBinaryVersion + '.6'
    sparkVersion = '1.6.1'
    snappySparkVersion = '1.6.2-6'
    sparkDistName = "spark-${sparkVersion}-bin-hadoop2.4"
    log4jVersion = '1.2.17'
    slf4jVersion = '1.7.12'
    junitVersion = '4.11'
    hadoopVersion = '2.4.1'
    jettyVersion = '8.1.14.v20131031'
    snappyStoreVersion = '1.5.1-SNAPSHOT'
    pulseVersion     = '1.5.0'
    buildFlags = ''
    createdBy = System.getProperty("user.name")
    osArch = System.getProperty('os.arch')
    osName = org.gradle.internal.os.OperatingSystem.current()
    osVersion = System.getProperty('os.version')
    buildDate = new Date().format('yyyy-MM-dd HH:mm:ss Z')
    buildNumber = new Date().format('MMddyy')
    jdkVersion = System.getProperty('java.version')

    gitCmd = "git --git-dir=${rootDir}/.git --work-tree=${rootDir}"
    gitBranch = "${gitCmd} rev-parse --abbrev-ref HEAD".execute().text.trim()
    commitId = "${gitCmd} rev-parse HEAD".execute().text.trim()
    sourceDate = "${gitCmd} log -n 1 --format=%ai".execute().text.trim()
  }

  if (!buildRoot.isEmpty()) {
    buildDir = new File(buildRoot, 'scala-' + scalaBinaryVersion + '/' +  project.path.replace(':', '/'))
  } else {
    // default output directory like in sbt/maven
    buildDir = 'build-artifacts/scala-' + scalaBinaryVersion
  }

  ext {
    testResultsBase = "${rootProject.buildDir}/tests/snappy"
    snappyProductDir = "${rootProject.buildDir}/snappy"
    sparkDistDir = "${rootProject.projectDir}/dist"
    sparkProductDir = "${sparkDistDir}/${sparkDistName}"
  }
}

def getProcessId() {
  def name = java.lang.management.ManagementFactory.getRuntimeMXBean().getName()
  return name[0..name.indexOf("@") - 1]
}

def getStackTrace(def t) {
  java.io.StringWriter sw = new java.io.StringWriter()
  java.io.PrintWriter pw = new java.io.PrintWriter(sw)
  org.codehaus.groovy.runtime.StackTraceUtils.sanitize(t).printStackTrace(pw)
  return sw.toString()
}

// Skip snappy-spark, snappy-aqp and spark-jobserver that have their own
// scalaStyle configuration. Skip snappy-store that will not use it.
configure(subprojects.findAll {!(it.name ==~ /snappy-spark.*/ ||
      it.name ==~ /snappy-store.*/ ||
      it.name ==~ /snappy-aqp.*/ ||
      it.name ==~ /spark-jobserver.*/)}) {
  scalaStyle {
    configLocation = "${rootProject.projectDir}/scalastyle-config.xml"
    inputEncoding = 'UTF-8'
    outputEncoding = 'UTF-8'
    outputFile = "${buildDir}/scalastyle-output.xml"
    includeTestSourceDirectory = false
    source = 'src/main/scala'
    testSource = 'src/test/scala'
    failOnViolation = true
    failOnWarning = false
  }
}

def cleanIntermediateFiles(def projectName) {
  def projDir = "${project(projectName).projectDir}"
  delete "${projDir}/metastore_db"
  delete "${projDir}/warehouse"
  delete "${projDir}/datadictionary"
  delete fileTree(projDir) {
    include 'BACKUPGFXD-DEFAULT-DISKSTORE**', 'locator*.dat'
  }
}
task cleanScalaTest << {
  def workingDir = "${testResultsBase}/scalatest"
  delete workingDir
  file(workingDir).mkdirs()
}
task cleanJUnit << {
  def workingDir = "${testResultsBase}/junit"
  delete workingDir
  file(workingDir).mkdirs()
}
task cleanDUnit << {
  def workingDir = "${testResultsBase}/dunit"
  delete workingDir
  file(workingDir).mkdirs()
  // clean spark cluster directories
  delete "${snappyProductDir}/work", "${snappyProductDir}/logs"
  delete "${sparkProductDir}/work", "${sparkProductDir}/logs"
}
task cleanAllReports << {
  def workingDir = "${testResultsBase}/combined-reports"
  delete workingDir
  file(workingDir).mkdirs()
}
task cleanQuickstart << {
  def workingDir = "${testResultsBase}/quickstart"
  delete workingDir
  file(workingDir).mkdirs()
}

task publishDocs(type:Exec) {
  dependsOn ':snappy-cluster_' + scalaBinaryVersion + ':docs'
  //on linux
  commandLine './publish-site.sh'
}

subprojects {
  // the run task for a selected sub-project
  task run(type:JavaExec) {
    if (!project.hasProperty('mainClass')) {
      main = 'io.snappydata.app.SparkSQLTest'
    } else {
      main = mainClass
    }
    if (project.hasProperty('params')) {
      args = params.split(",") as List
    }
    classpath = sourceSets.main.runtimeClasspath + sourceSets.test.runtimeClasspath
    jvmArgs '-Xmx2g', '-XX:MaxPermSize=512m'
  }

  task scalaTest(type: Test) {
    actions = [ new com.github.maiflai.ScalaTestAction() ]
    // top-level default is single process run since scalatest does not
    // spawn separate JVMs
    maxParallelForks = 1
    maxHeapSize '4g'
    jvmArgs '-XX:+HeapDumpOnOutOfMemoryError','-XX:+UseConcMarkSweepGC', '-XX:+CMSClassUnloadingEnabled',  '-XX:MaxPermSize=512m', '-ea'
    testLogging.exceptionFormat = 'full'

    List<String> suites = []
    extensions.add(com.github.maiflai.ScalaTestAction.SUITES, suites)
    extensions.add('suite', { String name -> suites.add(name) } )
    extensions.add('suites', { String... name -> suites.addAll(name) } )

    def result = new StringBuilder()
    extensions.add(com.github.maiflai.ScalaTestAction.TESTRESULT, result)
    extensions.add('testResult', { String name -> result.append(name) } )

    def output = new StringBuilder()
    extensions.add(com.github.maiflai.ScalaTestAction.TESTOUTPUT, output)
    extensions.add('testOutput', { String name -> output.append(name) } )

    def errorOutput = new StringBuilder()
    extensions.add(com.github.maiflai.ScalaTestAction.TESTERROR, errorOutput)
    extensions.add('testError', { String name -> errorOutput.append(name) } )

    // running a single scala suite
    if (rootProject.hasProperty('singleSuite')) {
      suite singleSuite
    }
    workingDir = "${testResultsBase}/scalatest"

    testResult '/dev/tty'
    testOutput "${workingDir}/output.txt"
    testError "${workingDir}/error.txt"
    binResultsDir = file("${workingDir}/binary/${project.name}")
    reports.html.destination = file("${workingDir}/html/${project.name}")
    reports.junitXml.destination = file(workingDir)
  }
  test {
    maxParallelForks = (2 * Runtime.getRuntime().availableProcessors())
    maxHeapSize '2g'
    jvmArgs '-XX:+HeapDumpOnOutOfMemoryError','-XX:+UseConcMarkSweepGC', '-XX:+CMSClassUnloadingEnabled',  '-XX:MaxPermSize=384m', '-ea'
    testLogging.exceptionFormat = 'full'

    include '**/*.class'
    exclude '**/*DUnitTest.class'
    exclude '**/*TestBase.class'

    workingDir = "${testResultsBase}/junit"

    binResultsDir = file("${workingDir}/binary/${project.name}")
    reports.html.destination = file("${workingDir}/html/${project.name}")
    reports.junitXml.destination = file(workingDir)
  }
  task dunitTest(type: Test) {
    dependsOn ':cleanDUnit'
    dependsOn ':product'
    maxParallelForks = 1
    minHeapSize '128m'
    maxHeapSize '1g'

    jvmArgs = ['-XX:+HeapDumpOnOutOfMemoryError', '-XX:MaxPermSize=384m', '-XX:+UseParNewGC', '-XX:+UseConcMarkSweepGC', '-XX:CMSInitiatingOccupancyFraction=50', '-XX:+CMSClassUnloadingEnabled', '-ea']

    include '**/*DUnitTest.class'
    exclude '**/*Suite.class'

    workingDir = "${testResultsBase}/dunit"

    binResultsDir = file("${workingDir}/binary/${project.name}")
    reports.html.destination = file("${workingDir}/html/${project.name}")
    reports.junitXml.destination = file(workingDir)

    systemProperties 'java.net.preferIPv4Stack': 'true',
                     'SNAPPY_HOME': snappyProductDir
  }

  gradle.taskGraph.whenReady({ graph ->
    tasks.withType(Jar).each { pack ->
      if (pack.name == 'packageTests') {
        pack.from(pack.project.sourceSets.test.output.classesDir, sourceSets.test.resources.srcDirs)
      }
    }
    tasks.withType(Test).each { test ->
      test.configure {

        def logLevel = System.getProperty('logLevel')
        if (logLevel != null && logLevel.length() > 0) {
          systemProperties 'gemfire.log-level'           : logLevel,
                           'logLevel'                    : logLevel
        }
        logLevel = System.getProperty('securityLogLevel')
        if (logLevel != null && logLevel.length() > 0) {
          systemProperties 'gemfire.security-log-level'  : logLevel,
                           'securityLogLevel'            : logLevel
        }

        environment 'SNAPPY_HOME': snappyProductDir,
          'APACHE_SPARK_HOME': sparkProductDir,
          'SNAPPY_DIST_CLASSPATH': "${sourceSets.test.runtimeClasspath.asPath}"

        def eol = System.getProperty('line.separator')
        beforeTest { desc ->
          def now = new Date().format('yyyy-MM-dd HH:mm:ss.SSS Z')
          def progress = new File(workingDir, "progress.txt")
          def output = new File(workingDir, "output.txt")
          progress << "${now} Starting test ${desc.className} ${desc.name}${eol}"
          output << "${now} STARTING TEST ${desc.className} ${desc.name}${eol}${eol}"
        }
        onOutput { desc, event ->
          def output = new File(workingDir, "output.txt")
          def msg = event.message
          if (event.destination.toString() == 'StdErr') {
            msg = msg.replace("\n", "\n[error]  ")
          }
          output << msg
        }
        afterTest { desc, result ->
          def now = new Date().format('yyyy-MM-dd HH:mm:ss.SSS Z')
          def progress = new File(workingDir, "progress.txt")
          def output = new File(workingDir, "output.txt")
          progress << "${now} Completed test ${desc.className} ${desc.name} with result: ${result.resultType}${eol}"
          output << "${eol}${now} COMPLETED TEST ${desc.className} ${desc.name} with result: ${result.resultType}${eol}${eol}"
          result.exceptions.each { t ->
            progress << "  EXCEPTION: ${getStackTrace(t)}${eol}"
            output << "${getStackTrace(t)}${eol}"
          }
        }
      }
    }
  })

  // apply default manifest
  if (rootProject.hasProperty('enablePublish')) {
    createdBy = "SnappyData Build Team"
  }
  jar {
    manifest {
      attributes(
        "Manifest-Version"  : "1.0",
        "Created-By"        : createdBy,
        "Title"             : rootProject.name,
        "Version"           : version,
        "Vendor"            : "SnappyData, Inc."
      )
    }
  }

  configurations {
    provided {
      description 'a dependency that is provided externally at runtime'
      visible true
    }

    testOutput {
      extendsFrom testCompile
      description 'a dependency that exposes test artifacts'
    }
    /*
    all {
      resolutionStrategy {
        // fail eagerly on version conflict (includes transitive dependencies)
        // e.g. multiple different versions of the same dependency (group and name are equal)
        failOnVersionConflict()
      }
    }
    */
  }

  task packageTests(type: Jar, dependsOn: testClasses) {
    description 'Assembles a jar archive of test classes.'
    classifier = 'tests'
  }
  artifacts {
    testOutput packageTests
  }

  idea {
    module {
      scopes.PROVIDED.plus += [ configurations.provided ]
    }
  }

  sourceSets {
    main.compileClasspath += configurations.provided
    main.runtimeClasspath -= configurations.provided
    test.compileClasspath += configurations.provided
    test.runtimeClasspath += configurations.provided
  }

  javadoc.classpath += configurations.provided

  dependencies {
    compile 'log4j:log4j:' + log4jVersion
    compile 'org.slf4j:slf4j-api:' + slf4jVersion
    compile 'org.slf4j:slf4j-log4j12:' + slf4jVersion

    testCompile "junit:junit:${junitVersion}"
  }
}

// maven publish tasks
subprojects {

  apply plugin: 'signing'

  task packageSources(type: Jar, dependsOn: classes) {
    classifier = 'sources'
    from sourceSets.main.allSource
  }
  task packageDocs(type: Jar, dependsOn: javadoc) {
    classifier = 'javadoc'
    from javadoc
  }
  if (rootProject.hasProperty('enablePublish')) {
    signing {
      sign configurations.archives
    }

    uploadArchives {
      repositories {
        mavenDeployer {
          beforeDeployment { MavenDeployment deployment -> signing.signPom(deployment) }

          repository(url: "https://oss.sonatype.org/service/local/staging/deploy/maven2/") {
            authentication(userName: ossrhUsername, password: ossrhPassword)
          }
          snapshotRepository(url: "https://oss.sonatype.org/content/repositories/snapshots/") {
            authentication(userName: ossrhUsername, password: ossrhPassword)
          }

          pom.project {
            name 'SnappyData'
            packaging 'jar'
            // optionally artifactId can be defined here
            description 'SnappyData distributed data store and execution engine'
            url 'http://www.snappydata.io'

            scm {
              connection 'scm:git:https://github.com/SnappyDataInc/snappydata.git'
              developerConnection 'scm:git:https://github.com/SnappyDataInc/snappydata.git'
              url 'https://github.com/SnappyDataInc/snappydata'
            }

            licenses {
              license {
                name 'The Apache License, Version 2.0'
                url 'http://www.apache.org/licenses/LICENSE-2.0.txt'
              }
            }

            developers {
              developer {
                id 'smenon'
                name 'Sudhir Menon'
                email 'smenon@snappydata.io'
              }
            }
          }
        }
      }
    }
  }
}

task publishLocal {
  dependsOn subprojects.findAll { p -> p.name != 'gemfirexd-native' &&
    p.name != 'gemfirexd-prebuild' &&
    p.name != 'gemfirexd' }.collect {
      it.getTasksByName('install', false).collect { it.path }
  }
}

task publishMaven {
  dependsOn subprojects.findAll { p -> p.name != 'gemfirexd-native' &&
    p.name != 'gemfirexd-prebuild' &&
    p.name != 'snappy-store' && p.name != 'gemfirexd' }.collect {
      it.getTasksByName('uploadArchives', false).collect { it.path }
  }
}


task generateSources {
  dependsOn ':snappy-spark:snappy-spark-streaming-flume-sink_' + scalaBinaryVersion + ':generateAvroJava'
  dependsOn ':snappy-store:generateSources'
}

task product {
  dependsOn ":snappy-core_${scalaBinaryVersion}:shadowJar"
  dependsOn ":snappy-cluster_${scalaBinaryVersion}:shadowJar"
  dependsOn ":snappy-examples_${scalaBinaryVersion}:jar"
  dependsOn ":snappy-cluster_${scalaBinaryVersion}:generateSnappydataZeppelinInterpreterJar"
  if (new File(rootDir, 'aqp/build.gradle').exists()) {
    dependsOn ":snappy-aqp_${scalaBinaryVersion}:jar"
  }

  doFirst {
    delete snappyProductDir
    file("${snappyProductDir}/lib").mkdirs()
  }
  doLast {
    // create the RELEASE file
    def release = file("${snappyProductDir}/RELEASE")
    def gitCommitId = "git rev-parse HEAD".execute().text.trim()
    release << "SnappyData ${project.version} ${gitCommitId} built for Hadoop $hadoopVersion\n"
    release << "Build flags: ${buildFlags}\n"

    // copy LICENSE, README.md and doc files
    copy {
      from projectDir
      into snappyProductDir
      include 'LICENSE'
      include 'README.md'
    }
    copy {
      from "${projectDir}/docs"
      into "${snappyProductDir}/docs"
    }

    def coreProject = project(":snappy-core_${scalaBinaryVersion}")
    def clusterProject = project(":snappy-cluster_${scalaBinaryVersion}")
    def examplesProject = project(":snappy-examples_${scalaBinaryVersion}")
    def coreName = "snappydata-core_${scalaBinaryVersion}-${version}.jar"
    def baseName = 'snappydata-assembly'
    def archiveName = "${baseName}_${scalaBinaryVersion}-${version}-hadoop${hadoopVersion}.jar"
    def exampleArchiveName = "quickstart-${version}.jar"
    // first copy the snappy-core shadow jar
    copy {
      from("${coreProject.buildDir}/libs")
      into "${rootProject.buildDir}/distributions"
      include "${coreProject.shadowJar.archiveName}"
      rename { filename -> coreName }
    }
    // next the full product starting with snappy-cluster shadow jar
    copy {
      from("${clusterProject.buildDir}/libs")
      into "${snappyProductDir}/lib"
      include "${clusterProject.shadowJar.archiveName}"
      rename { filename -> archiveName }
    }
    // copy datanucleus jars specifically since they don't work as part of fat jar
    // copy bin, sbin, data etc from spark
    if (new File(rootDir, 'spark/build.gradle').exists()) {
      copy {
        from project(":snappy-spark:snappy-spark-hive_${scalaBinaryVersion}").configurations.runtime.filter {
          it.getName().contains('datanucleus')
        }
        into "${snappyProductDir}/lib"
      }
      copy {
        from "${project(':snappy-spark').projectDir}/bin"
        into "${snappyProductDir}/bin"
      }
      copy {
        from "${project(':snappy-spark').projectDir}/sbin"
        into "${snappyProductDir}/sbin"
      }
      copy {
        from "${project(':snappy-spark').projectDir}/conf"
        into "${snappyProductDir}/conf"
      }
      copy {
        from "${project(':snappy-spark').projectDir}/python"
        into "${snappyProductDir}/python"
      }
      copy {
        from "${rootDir}/python"
        into "${snappyProductDir}/python"
      }
      copy {
        from("${examplesProject.projectDir}/src/main/python")
        into  "${snappyProductDir}/python/examples"
      }
      copy {
        from "${project(':snappy-spark').projectDir}/data"
        into "${snappyProductDir}/data"
      }

      def sparkR = "${project(':snappy-spark').projectDir}/R/lib/SparkR"
      if (file(sparkR).exists()) {
        copy {
          from sparkR
          into "${snappyProductDir}/R/lib"
        }
      }
    } else {
      copy {
        from examplesProject.configurations.testRuntime.filter {
          it.getName().contains('datanucleus')
        }
        into "${snappyProductDir}/lib"
      }
      def snappySparkJar = examplesProject.configurations.testRuntime.findResult {
        it.getName().startsWith("snappy-spark-${snappySparkVersion}") ? it.getPath() : null
      }
      if (snappySparkJar != null) {
        copy {
          from zipTree(snappySparkJar)
          into snappyProductDir
          include 'bin/**'
          include 'sbin/**'
          include 'conf/**'
          include 'python/**'
          include 'data/**'
          include 'R/**'
        }
      }
    }
    if (new File(rootDir, 'store/build.gradle').exists()) {
      // copy snappy-store shared libraries for optimized JNI calls
      copy {
        from "${project(':snappy-store:gemfirexd-core').projectDir}/lib"
        into "${snappyProductDir}/lib"
      }
      copy {
        from "${project(':snappy-store:gemfirexd-core').projectDir}/../quickstart"
        into "${snappyProductDir}/quickstart/store"
        exclude '.git*'
      }
      // also copy the JDBC client jar separately
      def clientProject = project(':snappy-store:gemfirexd-client')
      copy {
        from clientProject.jar.destinationDir
        into "${snappyProductDir}/lib"
        include clientProject.jar.archiveName
      }
    }
    // copy AQP jar
    if (new File(rootDir, 'aqp/build.gradle').exists()) {
      def aqpProject = project(":snappy-aqp_${scalaBinaryVersion}")
      copy {
        from("${aqpProject.buildDir}/libs")
        into "${snappyProductDir}/lib"
        include "${aqpProject.jar.archiveName}"
      }
    } else {
      copy {
        from examplesProject.configurations.testRuntime.filter {
          it.getName().contains('snappy-aqp')
        }
        into "${snappyProductDir}/lib"
      }
    }
    copy {
      from "${examplesProject.buildDir}/libs"
      into "${snappyProductDir}/lib"
      include "${examplesProject.jar.archiveName}"
      rename { filename -> exampleArchiveName }
    }
    copy {
      from("${clusterProject.projectDir}/bin")
      into "${snappyProductDir}/bin"
    }
    copy {
      from("${clusterProject.projectDir}/sbin")
      into "${snappyProductDir}/sbin"
    }
    copy {
      from("${clusterProject.projectDir}/conf")
      into "${snappyProductDir}/conf"
    }
    copy {
      from("${examplesProject.projectDir}/quickstart")
      into "${snappyProductDir}/quickstart"
    }
    copy {
      from("${clusterProject.projectDir}/benchmark")
      into "${snappyProductDir}/benchmark"
    }
    copy {
      from("${clusterProject.projectDir}/ec2")
      into "${snappyProductDir}/ec2"
      exclude "deploy/home/ec2-user/snappydata/servers"
      exclude "deploy/home/ec2-user/snappydata/leads"
      exclude "deploy/home/ec2-user/snappydata/locators"
      exclude "deploy/home/ec2-user/snappydata/snappy-env.sh"
      exclude "lib"
    }
  }
}

if (rootProject.hasProperty('copyToDir')) {
  task copyProduct(type: Copy, dependsOn: product) {
    from snappyProductDir
    into copyToDir
  }
}

// TODO: right now just copying over the product contents.
// Can flip it around and let distribution do all the work.

distributions {
  main {
    baseName = 'snappydata'
    contents {
      from { snappyProductDir }
    }
  }
}
distTar {
  dependsOn product
  // also package pulse and VSD
  dependsOn ':packagePulse', ':packageVSD'
  classifier 'bin'
}
distZip {
  dependsOn product
  // also package pulse and VSD
  dependsOn ':packagePulse', ':packageVSD'
  classifier 'bin'
}

// use the task below to prepare final release bits
task distProduct {
  dependsOn product, distTar, distZip
}

def copyTestsCommonResources(def bdir) {
  def outdir = "${bdir}/resources/test"
  file(outdir).mkdirs()

  copy {
    from "${rootDir}/tests/common/src/main/resources"
    into outdir
  }
}

def runScript(def execName, def workDir, def param) {
  def stdout = new ByteArrayOutputStream()
  exec {
    executable "${execName}"
    workingDir = workDir
    args (param)
    standardOutput = stdout
    environment 'PYTHONPATH', "${snappyProductDir}/python/lib/py4j-0.9-src.zip:${snappyProductDir}/python"
  }
  return "$stdout"
}

def runSQLScript(def fileName, def workDir) {
  println("Executing ${fileName}")
  def queryoutput = runScript("${snappyProductDir}/bin/snappy-shell", workDir,
          ["run", "-file=${snappyProductDir}/quickstart/scripts/${fileName}"]);
  println "${queryoutput}"
  if (queryoutput.contains("ERROR") || queryoutput.contains("Failed")) {
    throw new GradleException("Failed to run ${fileName}")
  }
}
def writeToFile(def fileName) {
  new File("$fileName").withWriter { out ->
    out.println "localhost"
    out.println "localhost"
  }
}

def runSubmitQuery(def jobName, def appName, def workDir) {
  println "Running job $jobName"
  def exampleArchiveName = "quickstart-${version}.jar"
  def submitjobairline = runScript("${snappyProductDir}/bin/snappy-job.sh", workDir,
      ["submit", "--lead", "localhost:8090", "--app-name", appName + System.currentTimeMillis(),
       "--class", jobName, "--app-jar",
       "${snappyProductDir}/lib/${exampleArchiveName}"]);

  println "${submitjobairline}"

  def jsonStr = (submitjobairline.charAt(2) == '{') ? submitjobairline.substring(2) :
      submitjobairline.substring(4)
  def json = new JsonSlurper().parseText(jsonStr)
  def jobid = ""
  json.each {
    k, v ->
      if (k == "result") {
        if (v instanceof groovy.json.internal.LazyMap) {
          jobid = v.get("jobId")
        }
      }
  }

  def status = "RUNNING"
  while (status == "RUNNING") {
    Thread.sleep(3000)
    def statusjobairline = runScript("${snappyProductDir}/bin/snappy-job.sh",
        workDir, ["status", "--lead", "localhost:8090", "--job-id", "${jobid}"]);
    println "${statusjobairline}"

    def statusjson = new JsonSlurper().parseText("${statusjobairline}")
    statusjson.each {
      k, v ->
        if (k == "status") {
          println("Current status of job: " + v)
          status = v
        }
    }
  }
  if (status == "ERROR") {
    throw new GradleException("Failed to submit queries")
  }
}

task runQuickstart {
  dependsOn cleanQuickstart
  dependsOn product
  mustRunAfter 'buildAll'
  def exampleArchiveName = "quickstart-${version}.jar"
  def workingDir = "${testResultsBase}/quickstart"
  doLast {
    try {
      writeToFile("${snappyProductDir}/conf/servers")
      def startoutput = runScript("${snappyProductDir}/sbin/snappy-start-all.sh",
          workingDir, []);
      println "${startoutput}"
      if (!startoutput.contains("Distributed system now has 4 members")) {
        throw new GradleException("Failed to start Snappy cluster")
      }
      runSQLScript("create_and_load_column_table.sql", workingDir)

      runSQLScript("create_and_load_row_table.sql", workingDir)

      runSQLScript("create_and_load_sample_table.sql", workingDir)

      runSQLScript("status_queries.sql", workingDir)

      runSQLScript("olap_queries.sql", workingDir)

      runSQLScript("oltp_queries.sql", workingDir)

      runSQLScript("olap_queries.sql", workingDir)

      runSQLScript("olap_approx_queries.sql", workingDir)

      runSubmitQuery("io.snappydata.examples.CreateAndLoadAirlineDataJob",
          "createJob", workingDir)

      runSubmitQuery("io.snappydata.examples.AirlineDataJob",
          "processjob", workingDir)

      def startSparkResult = runScript("${snappyProductDir}/sbin/start-all.sh",
          workingDir, [])
      println "${startSparkResult}"

      def hostname = "hostname".execute().text.trim()
      def airlineappresult = runScript("${snappyProductDir}/bin/spark-submit", workingDir,
              ["--class", "io.snappydata.examples.AirlineDataSparkApp", "--master", "spark://${hostname}:7077",
               "--conf", "snappydata.store.locators=localhost:10334", "--conf", "spark.ui.port=4041",
               "${snappyProductDir}/lib/${exampleArchiveName}"]);

      println "${airlineappresult}"
      if (airlineappresult.toLowerCase().contains("exception")) {
        throw new GradleException("Failed to submit AirlineDataSparkApp")
      }

      def examplesdir = project(":snappy-examples_${scalaBinaryVersion}")

      def airlinepythonappresult = runScript("${snappyProductDir}/bin/spark-submit", workingDir,
          ["--master", "spark://${hostname}:7077", "--conf", "snappydata.store.locators=localhost:10334",
           "--conf", "spark.ui.port=4042", "${snappyProductDir}/python/examples/AirlineDataPythonApp.py"]);

      println "${airlinepythonappresult}"
      if (airlinepythonappresult.toLowerCase().contains("exception")) {
        throw new GradleException("Failed to submit airlinepythonappresult")
      }

    } finally {
      println runScript("${snappyProductDir}/sbin/snappy-stop-all.sh", workingDir, []);
      println runScript("${snappyProductDir}/sbin/stop-all.sh", workingDir, []);
      def conffile = new File("${snappyProductDir}/conf/servers")
      if (conffile.exists())
        conffile.delete()
    }
  }
}

task copyResourcesAll << {
  copyTestsCommonResources(project(":snappy-core_${scalaBinaryVersion}").buildDir)
  copyTestsCommonResources(project(":snappy-cluster_${scalaBinaryVersion}").buildDir)
  if (new File(rootDir, 'aqp/build.gradle').exists()) {
    copyTestsCommonResources(project(":snappy-aqp_${scalaBinaryVersion}").buildDir)
  }
}


task cleanAll {
  dependsOn getTasksByName('clean', true).collect { it.path }
}
task buildAll {
  dependsOn getTasksByName('assemble', true).collect { it.path }
  dependsOn getTasksByName('testClasses', true).collect { it.path }
  dependsOn distTar, distZip
  mustRunAfter cleanAll
}
task checkAll {
  dependsOn ":snappy-core_${scalaBinaryVersion}:check",
            ":snappy-cluster_${scalaBinaryVersion}:check"
  if (new File(rootDir, 'aqp/build.gradle').exists()) {
    dependsOn ":snappy-aqp_${scalaBinaryVersion}:check"
  }
  if (project.hasProperty('spark')) {
    dependsOn ':snappy-spark:check'
  }
  if (project.hasProperty('store')) {
    dependsOn ':snappy-store:check'
  }
  mustRunAfter buildAll
}
task allReports(type: TestReport) {
  description 'Combines the test reports.'
  dependsOn cleanAllReports
  destinationDir = file("${testResultsBase}/combined-reports")
  mustRunAfter checkAll
}
gradle.taskGraph.whenReady({ graph ->
  tasks.getByName('allReports').reportOn rootProject.subprojects.collect{ it.tasks.withType(Test) }.flatten()
})

def writeProperties(def parent, def name, def comment, def propsMap) {
  parent.exists() || parent.mkdirs()
  def writer = new File(parent, name).newWriter()
  def props = new Properties()
  propsMap.each { k, v -> props.setProperty(k, v.toString()) }
  try {
    props.store(writer, comment.toString())
    writer.flush()
  } finally {
    writer.close()
  }
}

task packagePulse << {
  String pulseWarName = "pulse-${pulseVersion}.war"
  String pulseDir = System.env.PULSEDIR

  if (pulseDir == null || pulseDir.length() == 0) {
    pulseDir = "${projectDir}/../pulse"
  }

  String pulseDistDir = "${pulseDir}/build-artifacts/linux/dist"
  if (file(pulseDistDir).canWrite()) {
    exec {
      executable "${pulseDir}/build.sh"
      workingDir = pulseDir
      args 'clean', 'build-all'
    }
    delete "${snappyProductDir}/lib/pulse.war"
    println ''
    println "Copying Pulse war from ${pulseDistDir} to ${snappyProductDir}/lib"
    println ''
    copy {
      from "${pulseDir}/build-artifacts/linux/dist"
      into "${snappyProductDir}/lib"
      include "${pulseWarName}"
      rename { filename -> "pulse.war" }
    }
  } else {
    println "Skipping Pulse due to unwritable ${pulseDistDir}"
  }
}

task packageVSD << {
  String thirdparty = System.env.THIRDPARTYDIR
  String vsdDir = ""

  if (thirdparty == null || thirdparty.length() == 0) {
    vsdDir = "${projectDir}/../thirdparty/vsd"
  } else {
    vsdDir = "${thirdparty}/vsd"
  }

  String vsdDistDir = "${vsdDir}/70/vsd"
  if (file(vsdDistDir).canWrite()) {
    println ''
    println "Copying VSD from ${vsdDistDir} to ${snappyProductDir}/vsd"
    println ''
    delete "${snappyProductDir}/vsd"
    copy {
      from vsdDistDir
      into "${snappyProductDir}/vsd"
    }
  } else {
    println "Skipping VSD due to unwritable ${vsdDistDir}"
  }
}

task sparkPackage {
  dependsOn ":snappy-core_${scalaBinaryVersion}:sparkPackage"
}

packagePulse.mustRunAfter product
packageVSD.mustRunAfter product

distTar.mustRunAfter clean, cleanAll
distZip.mustRunAfter clean, cleanAll
distProduct.mustRunAfter clean, cleanAll

task precheckin {
  dependsOn cleanAll, buildAll, checkAll, allReports, runQuickstart
  dependsOn ':snappy-spark:scalaStyle', ":snappy-cluster_${scalaBinaryVersion}:docs"
}
